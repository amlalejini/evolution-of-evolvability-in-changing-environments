#!/bin/bash
########## Define Resources Needed with SBATCH Lines ##########

#SBATCH --time=<<TIME_REQUEST>>          # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --array=<<ARRAY_ID_RANGE>>
#SBATCH --mem=<<MEMORY_REQUEST>>        # memory required per node - amount of memory (in bytes)
#SBATCH --job-name <<JOB_NAME>>         # you can give your job a name for easier identification (same as -J)
#SBATCH --account=<<ACCOUNT_NAME>>

########## Command Lines to Run ##########

JOB_SEED_OFFSET=<<JOB_SEED_OFFSET>>
SEED=$((JOB_SEED_OFFSET + SLURM_ARRAY_TASK_ID - 1))

EXEC=<<EXEC>>
# Setup all directories at the beginning
CONFIG_DIR=<<CONFIG_DIR>>
REPO_DIR=<<REPO_DIR>>
RUN_DIR=<<RUN_DIR>>

# Load correct environment
cd ${REPO_DIR}
./load-hpc-env.sh
# Activate python virtual environment
source ${REPO_DIR}/pyenv/bin/activate

mkdir -p ${RUN_DIR}
cd ${RUN_DIR}
cp ${CONFIG_DIR}/*.cfg .
cp ${CONFIG_DIR}/*.org .
#cp ${CONFIG_DIR}/*.py .
cp ${CONFIG_DIR}/${EXEC} .

######## Run experiment ########
cd ${RUN_DIR}

<<RUN_COMMANDS>>

<<ANALYSIS_COMMANDS>>

######## Clean up ########

mv *.csv ./data/
rm ${RUN_DIR}/*.cfg
rm ${RUN_DIR}/*.org
#rm ${RUN_DIR}/*.py
rm ${RUN_DIR}/${EXEC}


